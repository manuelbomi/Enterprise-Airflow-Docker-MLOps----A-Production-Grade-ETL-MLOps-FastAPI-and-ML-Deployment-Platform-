# Enterprise Airflow Docker MLOps: A Production Grade ETL, MLOps, FastAPI, and ML Deployment Platform

#### Enterprise Airflow Docker MLOps is a production-grade, modular platform that demonstrates how to orchestrate end-to-end data engineering and MLOps workflows using Apache Airflow (TaskFlow API), Docker, Airflow Xcom and FastAPI.

Designed with enterprise use cases in mind, this project showcases how to:

- Build and manage ETL pipelines for data transformation

- Automate machine learning model training, validation, and deployment

- Leverage CI/CD pipelines to enforce quality and maintain workflow integrity

- Enable real-time model inference via FastAPI

- Use Airflowâ€™s advanced features like XCom, backfilling, catchup, and custom plugins

- Ensure model reliability through baseline validation, model versioning, and registry logic

* Whether your team is building AI/ML pipelines, automating data workflows, or deploying models in enterprise production environments, this project provides a battle-tested starting point that is extensible, containerized, and CI/CD-ready.

--- 

## Tech Stack  ğŸ§° 

- Airflow (TaskFlow API) â€“ Workflow orchestration, ETL, ML pipelines

- FastAPI â€“ High-performance model serving

- MLflow â€“ Model tracking, registry, and metrics logging

- Scikit-learn â€“ Training regression models

- Pandas/Numpy â€“ Data transformation

- Docker + Docker Compose â€“ Containerized orchestration of services

- GitHub Actions (CI/CD) â€“ Continuous Integration & DAG validation

- XCom â€“ Task-to-task communication in Airflow

- Production-grade design â€“ Model registry, validation, deployment, monitoring hooks

---

## Project Structure  ğŸ“ 


```ruby
enterprise-airflow-docker-mlops/
â”‚
â”œâ”€â”€ dags/                        # Airflow DAGs (various examples)
â”‚   â”œâ”€â”€ mlops_pipeline_taskflow.py
â”‚   â”œâ”€â”€ ml_pipeline_with_taskflow_api.py
â”‚   â”œâ”€â”€ data_engineering_etl_with_taskflow_api.py
â”‚   â”œâ”€â”€ data_engineering_etl_pipeline.py
â”‚   â”œâ”€â”€ dag_with_taskflow_api.py
â”‚   â”œâ”€â”€ ml_pipeline_with_traditional_dag.py
â”‚   â”œâ”€â”€ dags_with_catchup_and_backfill.py
â”‚   â””â”€â”€ xcom_dag.py
â”‚
â”œâ”€â”€ fastapi_app/
â”‚   â”œâ”€â”€ fastapi_app.py           # Model inference API
â”‚   â””â”€â”€ Dockerfile               # Optional: Standalone FastAPI Dockerfile
â”‚
â”œâ”€â”€ .github/workflows/
â”‚   â””â”€â”€ ci.yml                   # DAG validation, test automation
â”‚
â”œâ”€â”€ config/                      # (Optional) Airflow/ML configs
â”œâ”€â”€ logs/                        # Airflow logs
â”œâ”€â”€ models/                      # Persisted production models (served by FastAPI)
â”œâ”€â”€ plugins/                     # Custom plugins, sensors, operators
â”œâ”€â”€ project_snapshots/          # Snapshots of data/models
â”œâ”€â”€ test/                        # Unit tests (DAGs, FastAPI, ML)
â”‚   â”œâ”€â”€ test_dags.py
â”‚   â”œâ”€â”€ test_fastapi.py
â”‚   â””â”€â”€ test_model.py
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ docker-compose.yaml          # Compose file for running the full system
â”œâ”€â”€ Dockerfile.airflow           # Custom Airflow image
â””â”€â”€ README.md                    # You are currently here! Note that I did not include it in the original repo structure


```
---

## Key Features âœ…

 - TaskFlow API: Simplifies DAG writing using Python-native functions and decorators

 - ETL Pipelines: Easily build data engineering workflows with Airflow

-  ML Pipelines: End-to-end training, validation, and registration with MLflow

- MLOps Support: Baseline validation, model versioning, monitoring, and registry

- XCom Communication: Task-to-task data passing using native Airflow constructs

-  FastAPI Serving: Serve models via REST endpoint (http://localhost:8000)

-  CI/CD with GitHub Actions: DAG validation, linting, testing on push/PR

- Docker-based Orchestration: Easy spin-up of Airflow, Scheduler, UI, and FastAPI

- Production-Ready Hooks: Deployment logic, model monitoring stubs, custom registry

- Pluggable Architecture: Easily extendable via plugins, sensors, hooks

---


## Use Cases

Use this project as a blueprint for building real-world, enterprise-grade pipelines:

### Data Engineering & ETL ğŸ” 

- Build extract-transform-load (ETL) workflows using Airflow DAGs

- Use both traditional and TaskFlow API styles

- Backfill historical data using Airflow's catchup feature

### Machine Learning Pipelines  ğŸ§  

- Train and validate ML models (e.g., Linear Regression)

- Log metrics and models to MLflow

- Register and version models

### MLOps â€“ End-to-End Lifecycle ğŸ”

- Validate model performance with production baseline (auto-fails DAG if worse)

- Register models to a simulated model registry

- Deploy best model to production (via FastAPI)

- Stub in for model monitoring and drift detection

### Model Deployment  ğŸš€

- Automatically deploy latest validated model to /models/model.pkl

- FastAPI reads this for real-time predictions

### FastAPI Inference API  

- Live at http://localhost:8000

- Sample endpoints:
  
    - GET / â€” Health check

    - POST /predict â€” Send JSON input for predictions
      
      ```ruby
      curl -X POST "http://localhost:8000/predict" -H "Content-Type: application/json" \
      -d '{"x": 0.85}'        
      ```


### CI/CD Pipeline ğŸ”

- GitHub Actions workflow in .github/workflows/ci.yml

- Runs on PR and push:
  
      - DAG validation

      - Linting

      - Unit tests for ML and API

- Ready to extend with deployment hooks
    



--- 

## How to Deploy for Your Enterprise Application

### 1. Clone the Repo  ğŸ”

      
```ruby
git clone https://github.com/manuelbomi/Enterprise-Airflow-Docker-MLOps----A-Production-Grade-ETL-MLOps-FastAPI-and-ML-Deployment-Platform-/tree/main
cd enterprise-airflow-docker-mlops

```

### 2. Build & Run with Docker Compose  2ï¸âƒ£

####  Build custom images

```ruby
docker compose build
```

#### Spin up the full stack: Airflow + FastAPI

```ruby
docker compose up
```

#### Visit  ğŸ“Œ :  

<ins>Airflow UI</ins>: http://localhost:8080

<ins>FastAPI Server</ins>: http://localhost:8000



### 3. Run Your First DAG  3ï¸âƒ£ 

    - Go to Airflow UI: http://localhost:8080
    
    - Unpause mlops_pipeline_taskflow_emm_oye_v3

    - Trigger DAG manually or wait for the scheduler



#### FastAPI Endpoint Details    ğŸŒ 


| Method | Route      | Description                                                  |
|--------|------------|--------------------------------------------------------------|
| GET    | `/`        | Health check                                                 |
| POST   | `/predict` | Accepts JSON like `{"x": 0.5}` and returns model prediction  |


### 4. Running Tests

#### Run unit tests


```ruby
pytest test/

```

### 5.GitHub Actions (CI/CD)  âœ… 

* CI/CD workflow automatically:

    - Validates all DAGs

    - Runs unit tests

    -  Lints code (expandable)

    -  Blocks merging broken DAGs

* File: .github/workflows/ci.yml

---

## Extending the Platform   ğŸ“¦ 

* You can use this project as a template to build:

     - Production-grade data pipelines

     - MLOps workflows with real monitoring

     - Automated ML retraining & deployment

     - Data validation + data drift detection

     - Full ML/ETL lifecycle orchestration
 
 ---

## Notes  ğŸ“Œ 

    - Models are saved in /tmp/production_model/model.pkl (shared with FastAPI)

    - MLflow runs are saved locally at /tmp/mlruns

    - Model validation uses MSE vs a BASELINE_MSE

    - All DAGs are Pythonic, modular, and reusable







